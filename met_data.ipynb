{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from itertools import groupby\n",
    "\n",
    "import pandas as pd\n",
    "from geopy.geocoders import ArcGIS\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geocoding import geocode\n",
    "\n",
    "api_key = \"AAPK4d74de30452c4083822bac73c98a3c47pQecCTVCLZrikOxylcE0J26EvgdbFXBNOU-jPMjLdCF_GvHua3Aok7y0pmNoPdBf\"\n",
    "portal = GIS(\"https://www.arcgis.com\", api_key=api_key)\n",
    "\n",
    "raw_data = pd.read_csv(\"met_data_original.csv\", dtype = str)\n",
    "raw_data\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked all possible geotypes --> will create new dataframe with origins + coordinates of each location\n",
    "geo_df = raw_data[['Object Number', 'Object ID', 'Title', 'Geography Type', 'City',\t'State', 'County', 'Country', 'Region', 'Subregion']].copy()\n",
    "\n",
    "for i in geo_df.index:    \n",
    "\n",
    "    extract_loc = [geo_df[\"City\"].iloc[i], \n",
    "                    geo_df[\"Subregion\"].iloc[i], \n",
    "                    geo_df[\"State\"].iloc[i], \n",
    "                    geo_df[\"County\"].iloc[i]]\n",
    "\n",
    "    if type(geo_df[\"Country\"].iloc[i]) != float:\n",
    "        extract_loc.append(geo_df[\"Country\"].iloc[i])\n",
    "    elif type(geo_df[\"Region\"].iloc[i]) != float:\n",
    "        extract_loc.append(geo_df[\"Region\"].iloc[i])\n",
    "\n",
    "    base_loc = [p for p in extract_loc if not(pd.isnull(p)) == True]\n",
    "    base_loc = [s.replace('Mesopotamia', 'Iraq') for s in base_loc]\n",
    "    base_loc = [s.replace('Northern Syria', 'Syria') for s in base_loc]\n",
    "\n",
    "    rm_lines = ['|', '||']\n",
    "    base_loc =  [line for line in base_loc if line not in rm_lines]\n",
    "\n",
    "    joined_loc = []\n",
    "    for l in base_loc:\n",
    "        if \"|\" in l: \n",
    "            if len(base_loc) == 1:\n",
    "                split_loc = l.split(\"|\")\n",
    "                joined_loc = [key for key, _ in groupby(split_loc)]\n",
    "            else:\n",
    "                line_count = sum(s.count('|') for s in base_loc)\n",
    "                if line_count >= len(base_loc):\n",
    "                    l1 = []\n",
    "                    l2 = []\n",
    "                    for l in base_loc:\n",
    "                        catch = l.split('|')\n",
    "                        if len(catch) > 1:\n",
    "                            l1.append(catch[0])\n",
    "                            l2.append(catch[1])\n",
    "                    join1 = ', '.join(l1)\n",
    "                    join2 = ', '.join(l2)\n",
    "                    joined_loc = [join1, join2]\n",
    "        else:\n",
    "            if \" or \" in l or \" and \" in l and \"kingdom\" not in l:\n",
    "                print(base_loc)\n",
    "                index = base_loc.index(l)\n",
    "                ctxt = ', '.join(base_loc[index + 1:])\n",
    "                base_loc.remove(l)\n",
    "                if index == 0:\n",
    "                    split_loc = []\n",
    "                    if \" or \" in l:\n",
    "                        split_loc = l.split(\" or \")\n",
    "                    \n",
    "                    if \" and \" in l:\n",
    "                        split_loc = l.split(\" and \")\n",
    "            \n",
    "                    insert_pos = 0\n",
    "                    for w in split_loc:\n",
    "                        if len(ctxt) > 0:\n",
    "                            joined_loc.insert(index + insert_pos, w + \", \" + ctxt)\n",
    "                        else:\n",
    "                            joined_loc.insert(index + insert_pos, w)\n",
    "                        insert_pos += 1\n",
    "            else:\n",
    "                joined_loc.append(', '.join(base_loc))\n",
    "\n",
    "        for j in joined_loc:\n",
    "            print(j)\n",
    "            geo_df.loc[i, \"origin_\" + str(joined_loc.index(j))] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode each origin\n",
    "for i, row_loc in enumerate(geo_df['origin_0']):\n",
    "    if type(row_loc) != float:\n",
    "        try:\n",
    "            coords = geocode(address=row_loc)[0]['location']\n",
    "            geo_df.loc[i, \"lat_0\"] = coords['y']\n",
    "            geo_df.loc[i, \"long_0\"] = coords['x']\n",
    "            print(str(coords['y']) + ', ' + str(coords['x']))\n",
    "        except:\n",
    "            geo_df.loc[i, \"lat_0\"] = \"Unspecified\"\n",
    "            geo_df.loc[i, \"long_0\"] = \"Unspecified\"\n",
    "\n",
    "\n",
    "for i, row_loc in enumerate(geo_df['origin_1']):\n",
    "    if type(row_loc) != float:\n",
    "        try:\n",
    "            coords = geocode(address=row_loc)[0]['location']\n",
    "            geo_df.loc[i, \"lat_1\"] = coords['y']\n",
    "            geo_df.loc[i, \"long_1\"] = coords['x']\n",
    "            print(str(coords['y']) + ', ' + str(coords['x']))\n",
    "        except:\n",
    "            geo_df.loc[i, \"lat_1\"] = \"Unspecified\"\n",
    "            geo_df.loc[i, \"long_1\"] = \"Unspecified\"\n",
    "\n",
    "\n",
    "for i, row_loc in enumerate(geo_df['origin_2']):\n",
    "    if type(row_loc) != float:\n",
    "        try:\n",
    "            coords = geocode(address=row_loc)[0]['location']\n",
    "            geo_df.loc[i, \"lat_2\"] = coords['y']\n",
    "            geo_df.loc[i, \"long_2\"] = coords['x']\n",
    "            print(str(coords['y']) + ', ' + str(coords['x']))\n",
    "        except:\n",
    "            geo_df.loc[i, \"lat_2\"] = \"Unspecified\"\n",
    "            geo_df.loc[i, \"long_2\"] = \"Unspecified\"\n",
    "\n",
    "\n",
    "for i, loc in enumerate(geo_df['origin_3']):\n",
    "    if type(row_loc) != float:\n",
    "        try:\n",
    "            coords = geocode(address=row_loc)[0]['location']\n",
    "            geo_df.loc[i, \"lat_3\"] = coords['y']\n",
    "            geo_df.loc[i, \"long_3\"] = coords['x']\n",
    "            print(str(coords['y']) + ', ' + str(coords['x']))\n",
    "        except:\n",
    "            geo_df.loc[i, \"lat_3\"] = \"Unspecified\"\n",
    "            geo_df.loc[i, \"long_3\"] = \"Unspecified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for safety\n",
    "geo_df.to_csv('geodf_save.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin with original info\n",
    "geo_df.drop(['Object Number', 'Title', 'Geography Type', 'City', 'State', 'County', 'Country', 'Region', 'Subregion'], axis=1, inplace=True)\n",
    "joined_data = raw_data.merge(geo_df, on='Object ID')\n",
    "joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm unnecessary columns\n",
    "joined_data.drop(['Is Highlight', 'Is Timeline Work', 'Is Public Domain', 'Gallery Number', 'Dynasty', 'Reign', 'Portfolio', 'Constituent ID',\t'Artist Prefix', 'Artist Suffix', 'Artist Alpha Sort', 'Artist Begin Date',\t'Artist End Date',\t'Artist Gender', 'Artist ULAN URL', 'Artist Wikidata URL', 'Object Begin Date', 'Object End Date', 'Rights and Reproduction', 'Object Wikidata URL', 'Repository', 'Tags', 'Tags AAT URL', 'Tags Wikidata URL'], axis=1, inplace=True)\n",
    "joined_data = joined_data.dropna(axis=0, subset=['origin_0'])\n",
    "\n",
    "coords = geocode(address=\"Metropolitan Museum of Art, New York, NY\")[0]['location']\n",
    "joined_data['present_loc_lat'] = coords['y']\n",
    "joined_data['present_loc_long'] = coords['x']\n",
    "\n",
    "joined_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add link to images if available \n",
    "for i, row in enumerate(joined_data['Object ID']):\n",
    "    response = requests.get('https://collectionapi.metmuseum.org/public/collection/v1/objects/' + joined_data.loc[i, 'Object ID'])\n",
    "    get_info = response.json()\n",
    "    if 'primaryImageSmall' in get_info:\n",
    "        joined_data.loc[i, '<img>-tooltip'] = get_info['primaryImageSmall']\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "joined_data.to_csv('geocode_met_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('hist5705_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcd7a81f6b78e14bd81db77dcf0999d28996c25b78bbef59c121c9a4b8e10634"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
